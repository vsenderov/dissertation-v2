\chapter{Problem specification}
\label{chapter-problem-defintion}

As stated in Chapter~\ref{chapter-introduction}, the goal of the present Ph.D. effort is ``to create an open knowledge-based system of biodiversity information extracted from scholarly literature.'' Biodiversity data is quite heterogeneous and comes from many sources; for example, there is taxonomic data (data about the names and descriptions of species), bio-geographic data (data about occurrences of orgamisms at specific locations), genomic data (data about the genetic makeup of species) and so on. For a detailed domain conceptualization including a full discussion of the types of biodiversity data, please refer later to Chapter~\ref{chapter-ontology}. Due to this heterogeneity and in order to ensure the feasibility of the project as a Ph.D. thesis, the OpenBiodiv system that was developed is focused primarily on creating the models and infrastructure needed for processing scholarly publications of biological systematics and taxonomy. 

As per the publication \cite{noauthor_open_2014}, the system ought to meet criteria such as providing ``a consistent biodiversity information space,'' ``new formats to support novel and diverse uses,'' ``linkages with other resources,'' ``accreditation for researcher's work,'' and others. Deliberations about the system were published in in the pro-iBiosphere final report (\cite{soraya_sierra_coordination_2014}). However, the language of the report is high-level and does not provide a formal specification for the system but rather a set of recommendations on the features and implementation of the system.

In 2016, based on the outcomes of pro-iBiosphere and on the previous work in the area of biodiversity informatics (see Chapter~\ref{chapter-introduction}), we published the Ph.D. plan for this research (\cite{senderov_open_2016}). This publication can be considered as the first design specification of OpenBiodiv. However, in the course of developing the system, its design was changed iteratively through a feedback loop from collaborators from the \href{http://big4-project.eu}{BIG4 project}\footnote{The Ph.D. candidate, Viktor Senderov, is part of the Marie SkÅ‚odowska-Curie BIG4 International Training Network: Biosystematics, informatics and genomics of the big 4 insect groups: training tomorrow's researchers and entrepreneurs.} and various international collaborators. We view this positively and in the spirit of both open science and agile software development (\cite{beck_manifesto_2001}).

This chapter should serve, therefore, as an updated version the Ph.D. project plan (\cite{senderov_open_2016}) and as a specification and design blueprint for the OpenBiodiv system; subsequent chapters contain discussions of the implementation of particular components of the system.

\section{What is a knowledge-based system?}

We shall start by first introducing \emph{knowledge bases} and \emph{knowledge-based systems}.  We use the two terms interchangeably but tend to write the longer knowledge-based system when we want to emphasize aspects of the knowledge base that are not related to do with underlying facts store (database).

It is useful to form one's concept of knowledge-based systems both by looking at explicit definitions and by looking at several examples of knowledge bases in practice. The term was already being widely discussed by the 1980's (\cite{brodie_kbms_1989}) and early nineties (\cite{harris_knowledge_1993}) and was understood to mean the utilization of ideas from both database management systems (DBMS) and artificial intelligence (AI) to create a type of computer system called \emph{knowledge base management system} (KBMS). \cite{harris_knowledge_1993} writes that the characteristics of a knowledge base management system are that it contains ``prestored rules and facts from which useful inferences and conclusions may be drawn by an inference engine.''  We should note that the phrase ``prestored rules'' comes from the time of first-generation AI systems that were rule-based. Recently, there has been progress in incorporating statistical techniques into databases (\cite{mansinghka_bayesdb:_2015}); however, in this project we are working with the classical rule-based definition. In other words, a knowledge base is, in our understanding, a suitable database tightly integrated with a logic layer.

Another relatively more recent development in knowledge-based systems has been the application of Linked Data principles (\cite{heath_linked_2011}). In fact, most existing knowledge bases emphasize the community aspects of making data more interconnected and reusable. Examples include Freebase (\cite{bollacker_freebase:_2008}), which recently migrated to WikiData (\cite{vrandecic_wikidata:_2014, pellissier_tanon_freebase_2016}), DBPedia (\cite{hutchison_dbpedia:_2007}), as well as Wolfram|Alpha (\cite{wolfram_research_inc_wolfram|alpha_nodate}) and Google Knowledge Graph (\cite{singhal_introducing_2012}). What these systems have in common that an emphasis is placed not only on the logic layer allowing inference but on a unified information space: these systems act as nexus integrating information from multiple places following to various degrees the principles of Linked Open Data (LOD).

Linked Open Data (\cite{heath_linked_2011}) is a concept of the Semantic Web (\cite{berners-lee_semantic_2001}), which, when applied properly, ensures that data published on the Web is reusable, discoverable and most importantly ensures that pieces of data published by different entities can work together.  We will discuss the Linked Data principles and their application to OpenBiodiv in detail in Chapter~\ref{chapter-lod}.

In fact, modern knowledge bases place a bigger emphasis on interlinking data rather than on developing a complex inference machinery. There has been critique of the idea of bundling logic in the database layer as such bundling leads to increased complexity (\cite{barrasa_rdf_2017}). The critique can be summarized with two points. First, bundling the logic near the data (especially when it is excessive for the task at hand) can lead to drastic performance decreases \footnote{We will compare the performance of the stronger OWL logic layer with a weaker RDFS logic layer in Chapter~\ref{chapter-lod}.}. Second, the developing of new techniques (e.g. machine learning) can make the existing deep logic layer obsolete. Our view is that data is the commodity which is much more valuable, and the inference strategy (be it a rule-based logic layer, or a statistical machine learning technique) can be replaced as computational science moves forward. These ideas lead to an interesting conundrum in the choice of a database technology discussed in the subsequent sections.

Finally, a knowledge-based system ultimately needs to include user-interface components (UI's) and application programming interfaces (API's)---application layer.

\section{What is OpenBiodiv?}

The understanding of OpenBiodiv as a knowledge-based system can thus be summarized as follows: OpenBiodiv is a  database of interconnected biodiversity information together with a logic and application layers allowing users to not only query the data but also discover additional facts of relevance implied by the data. We now proceed to specify the technologies that are used to implement the system.

\section{Choice of database paradigm for OpenBiodiv}

We specify OpenBiodiv as a knowledge-based system with a focus on structuring and interlinking biodiversity data. Two of the possible database technologies that fit this requirement are semantic graph databases (triple stores) such as GraphDB (\cite{ontotext_graphdb_2018}) and labeled property graphs such as Neo4J (\cite{neo4j_developers_neo4j_2012}).  Semantic graph databases offer a very simple data model: every fact stored in such a database is composed as a triple of \emph{subject}, \emph{predicate}, and \emph{object}. Subjects of triples are always resource identifiers, whereas objects can be other resource identfiers or literal values (e.g. strings, numbers, etc.).  Links between resources or between resources and literals are given by the predicates (also specified as identifiers). These links are sometimes referred to as \emph{properties}. Thus, one can visualize a graph whose vertices are resource identifiers or literals and whose edges are predicates.

Semantic graph databases have the unique feature that the logic layer is also expressed as triples stored in the database. This logic layer, known as \emph{ontology}, is not only responsible for drawing conclusions from the data (inference), but also specifies the semantics of how knowledge should be expressed.

Labeled property graphs, on the other hand, offer a freer data model by allowing the edges of the knowledge graph to have properties as well. For example, in a labeled property graph whose vertices are two cities A and B and connected by a property-predicate \emph{connected by road}, it is possible to additionally attach the value ``500 km'' to that property. Thus we indicate that the length of the road connecting the cities is 500 km.

Note that labeled property graphs are not any more expressive that what can be achieved by triples alone. In fact, complex relationships in a simple triple store can be expressed by making relationships into nodes who have properties on their own. This process is known as \emph{reification}. For example, the two cities $A$ and $B$ can connect to a further vertex, $R$ indicating the road. $R$ will then have three properties: \emph{start}, \emph{end}, and \emph{length}. The value (object) of \emph{start} will be $A$, of \emph{end} will be $B$, and of length will be the literal ``500 km.''

We have summarized the differences between labeled property graphs and semantic graph databases in Table~\ref{graphdb-vs-neo4k}. After carefull considerations, we settled on the triple store, i.e. semantic graph database as a choice of database technology. This decision was informed by the wide availability of high-quality ontologies and Resource Description Framework (RDF) data models in our domain (\cite{baskauf_darwin-sw:_2016,peroni_semantic_2014}) and the popularity of the Semantic Web (\cite{berners-lee_semantic_2001}) in the community. Furthermore, our base at a publisher was more suited to a standards-driven foundational project as opposed to a particular application.

\begin{table}
\caption{Differences between semantic graph databases (e.g. GraphDB) and labeled property graphs (e.g. Neo4j).}
\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}|>{\centering\arraybackslash}m{4.2cm}|>{\centering\arraybackslash}m{4.2cm}}
Criterion   & Semantic database & Labeled property graph\\
\hline
Semantics   & Stored in the database itself as OWL or RDFS statements. Provides a uniform data space. Requires expert ontologists to extract knowledge.
            & Formal semantics usually are missing. Quick deployment. Uniform data space harder to achieve.\\
\hline
Inference   & Provided by the database itself from its ontology or expressed as SPARQL queries. General purpose, slower.
            & External to the database. Needs to be written for every specific task. Special purpose. Faster.\\
\hline
Community   & Has a rich and mature community of ontologists and knowledge engineers. Lots of domain ontologies. Designed for inter-operability. Standards-driven.
            & Data models are created ad-hoc by data scientists or programmers for a particular task. Inter-operability requires effort and not of primary concern. Applications-driven.\\
\hline
\end{tabular}
\label{graphdb-vs-neo4k}
\end{table}

However, we believe that labeled property graphs are a freer and more natural data model and are perfectly suited for biodiversity informatics. In particular they provide a much more natural formalism for relationships between taxonomic concepts (discussed in Chapter~\ref{chapter-ontology}). Also, non-RDF semantic databases such as \mbox{WikiData} are gaining in popularity. Therefore, we believe that the applicability of RDF triple stores for OpenBiodiv should constantly be reevaluated.

\section{Choice of information sources}

According to \cite{soraya_sierra_coordination_2014}, biodiversity and biodiversity-related data have two different ``life-cycles.'' In the past, after an observation of a living organism had been made, it was recorded on paper and then the observation record was published in paper-based form. In order for biodiversity data to be available to the modern scientist, efforts are made nowadays to digitize those paper-based publications by Plazi \cite{agosti_why_2007} and the Biodiversity Heritage Library (\cite{miller_taxonomic_2012}). For this purpose, several dedicated XML schemas have been developed (see \cite{penev_xml_2011} for a review), of which TaxPub (\cite{catapano_taxpub:_2010}) and TaxonX  seem to be the most widely used (\cite{penev_implementation_2012}). The digitization of publications contains several steps. After scanning and optical character recognition (OCR), text mining is combined with searching for particular kinds of data. This procedure leaves a trace in the form of marked-up (tagged) elements that can then be extracted and made available for future use and reuse (\cite{miller_integrating_2015}).

In present day, biodiversity data and publications are mostly ``born digital'' as semantically Enhanced Publications (EP's, \cite{claerbout_electronic_1992,godtsenhoven_van_emerging_2009,shotton_semantic_2009}). According to \cite{claerbout_electronic_1992}, ``an EP is a publication that is enhanced with research data, extra materials, post publication data and database records. It has an object-based structure with explicit links between the objects. An object can be (part of) an article, a data set, an image, a movie, a comment, a module or a link to information in a database.'' Semantically enhanced publications are thus natives of the Web and the Semantic Web unlike their paper-based predecessors.

The act of publishing in a digital, enhanced format, differs from the ground up from a paper-based publication. The main difference is that a digitally-published document can be structured in such a format as to be suitable both for machine processing and to the human eye. In the sphere of biodiversity science, Pensoft journals such as ZooKeys, PhytoKeys, and the Biodiversity Data Journal (BDJ) already function by providing EP's (\cite{penev_semantic_2010}).

Given the fact that Pensoft Publishers' and Plazi's publications cover a large part of taxonomic literature both in volume and also in temporal span, and the fact that the publications of those two publishers are available as semantic EP's, we've chosen Pensoft's journals and Plazi's treatments as our main sources of information.

Furthermore, we incorporate the taxonomic backbone of GBIF \cite{gbif_secretariat_gbif_2017} as a source for data integration. This is further discussed in Chapter~\ref{chapter-lod}.

\section{Choice of Programming Environment}

In recent years the R programming language has been used widely in the field of data science (\cite{r_core_team_r:_2016}). R has a rich library of software packages including such for processing XML (\cite{wickham_xml2:_2018}), for accessing rest API's (\cite{wickham_httr:_2017}) and focuses on open science \cite{boettiger_building_2015}. Furthermore, R is widely adopted in the biodiversity informatics community and I am proficient in it. For this reason, I have chosen the R software environment as the main programming environment.

\section{Methodologies: Open Science, Agile software development, Semantic Web}

After having specified the desired design---a semantic graph database of biodiversity information together with an application layer serving the needs of the biodiviersity community and extracted from taxonomic literature and GBIF---and given the programming language---R---I would like to discuss last some methodologies and frameworks that I have adopted to be more efficient, open, and reproducible.

We are of the opinion that the OpenBiodiv needs to be addressed from the point of view of \emph{Open Science}. According to \cite{kraker_case_2011} and to \cite{noauthor_was_nodate}, the six principles of open science are open methodology, open source, open data, open access, open peer review, and open educational resources. It is my belief that the aim of open science is to ensure access to the whole research product: data, discoveries, hypotheses, and so on. This opening-up will ensure that the scientific product is reproducible and verifiable by other scientists (\cite{mietchen_transformative_2014}). There is a very high interest in development of processes and instruments enabling reproducibility and verifiability, as can be evidenced for example by a special issue in Nature dedicated to reproducible research (\cite{noauthor_challenges_2010}). Therefore, the source code, data, and publications of OpenBiodiv will be published openly.

I also believe that OpenBiodiv should be thought of as integral part of the Semantic Web \cite{berners-lee_semantic_2001}. The Semantic Web is a vision for the future of the web where not only documents but also data are connected.

\section{Definition of OpenBiodiv as a Research Problem}

Following the objectives outlined in the previous chapter and the clarification of the definitions from this chapter, the research problem of OpenBiodiv can postulated as designing an open RDF semantic graph database, incorporating information stored in Pensoft, Plazi, and GBIF, and allowing the user of the system to ask complicated queries. 

As a blueprint for the type queries in the domain of biodiversity science that should be answerable with the help of the system, we have looked at the list compiled in \cite{pro-ibiosphere_competency_2013}. Examples include: "Is X a valid taxonomic name?" ``What are related names to a given name?'' ``Which authors have published about a given taxon?''

In the next chapter we discuss the design and overall architecture of OpenBiodiv and in the subsequent chapters we discuss the individual components.
