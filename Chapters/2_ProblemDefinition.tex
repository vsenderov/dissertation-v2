\chapter{Problem specification}
\label{chapter-problem-defintion}

As stated in Chapter~\ref{chapter-introduction}, the goal of the present Ph.D. effort is ``to create an open knowledge-based system of biodiversity information extracted from scholarly literature.'' Biodiversity data is quite heterogeneous and comes from many sources; for example, there is taxonomic data (data about the names and descriptions of species), bio-geographic data (data about occurrences of orgamisms at specific locations), genomic data (data about the genetic makeup of species) and so on. For a detailed domain conceptualization including a full discussion of the types of biodiversity data, please refer later to Chapter~\ref{chapter-ontology}. Due to this heterogeneity and in order to ensure the feasibility of the project as a Ph.D. thesis, the OpenBiodiv system that was developed is focused primarily on creating the models and infrastructure needed for processing scholarly publications of biological systematics and taxonomy. 

As per the ``Principles of OBKMS'' from the publication \cite{pro-ibiosphere_open_2014}, the system ought to meet criteria such as providing ``a consistent biodiversity information space,'' ``new formats to support novel and diverse uses,'' ``linkages with other resources,'' ``accreditation for researcher's work,'' and others. Considerations for the system were published in in the pro-iBiosphere final report (\cite{soraya_sierra_coordination_2014}). The language of the report is high-level and does not provide a formal specification for the system but rather a set of recommendations on the features and implementation of the system.

In 2016, based on the outcomes of pro-iBiosphere and on the main results in the area (see Chapter~\ref{chapter-introduction}), we published the Ph.D. plan for this thesis as a design specification for the system (\cite{senderov_open_2016}). However, in the course of developing the system, its design was changed iteratively through a feedback loop from collaborators from the BIG4 project consortium\footnote{The BIG4 project, \href{http://big4-project.eu}{<http://big4-project.eu>}.}, of which the author is a member of, and various international collaborators. We view this positively and in the spirit of both Open Science and the Manifesto for Agile Software Development\footnote{Manifesto for Agile Software Development \href{http://agilemanifesto.org/},{<http://agilemanifesto.org/>}.}.

Therefore, this chapter should serve as an updated version the Ph. D. project plan published in 2016 (\cite{senderov_open_2016}) and as a birds-eye of the OpenBiodiv system; subsequent chapters contain discussions of the particular components of the system.

\section{What is a knowledge-based system?}

We shall start by first introducing \emph{knowledge bases} and \emph{knowledge-based systems}.  We use the two terms interchangeably but tend to write the longer  ``knowledge-based system'' when we want to emphasize aspects of the knowledge base that are not related to do with underlying facts store (database).

It is useful to form one's concept of knowledge-based systems both by looking at explicit definitions and by looking at several examples of knowledge bases in practice. The term was already being widely discussed by the 1980's (\cite{brodie_kbms_1989}) and early nineties (\cite{harris_knowledge_1993}) and was understood to mean the utilization of ideas from both database management systems (DBMS) and artificial intelligence (AI) to create a type of computer system called knowledge base management system (KBMS). \cite{harris_knowledge_1993} writes that the characteristics of a KBMS are that it contains ``prestored rules and facts from which useful inferences and conclusions may be drawn by an inference engine.''  We should note that the phrase ``prestored rules'' comes from the time of first-generation AI systems that were rule-based. Recently, there has been progress in incorporating statistical techniques into databases (\cite{mansinghka_bayesdb:_2015}); however, in this project we are working with the classical rule-based definition.

Another relatively more recent development in knowledge-based systems has been the application of Linked Data principles (\cite{heath_linked_2011}). In fact, most existing knowledge bases emphasize the community aspects of making data more interconnected and reusable. Examples include Freebase (\cite{bollacker_freebase:_2008}), which recently migrated to WikiData (\cite{vrandecic_wikidata:_2014, pellissier_tanon_freebase_2016}), and DBPedia (\cite{hutchison_dbpedia:_2007}), as well as Wolfram, Google Knowledge Graph \todo{cite}.

Linked Open Data (LOD, \cite{heath_linked_2011}) is a concept of the Semantic Web (\cite{berners-lee_semantic_2001}) applied to ensure that data published on the Web is reusable, discoverable and most importantly to ensure that pieces of data published by different entities can work together.  We will discuss the Linked Data principles and their application to OpenBiodiv in detail in Chapter~\ref{chapter-lod}.

Together with the push to interlink data, there has been critique of the idea of bundling logic in the database layer as such bundling leads to increased complexity (\cite{barrasa_rdf_2017}). This leads to an interesting conundrum in the choice of a database technology discussed later in this chapter.

\section{What is OpenBiodiv?}

The understanding of OpenBiodiv as a knowledge-based system can thus be summarized as follows: OpenBiodiv is a  database of interconnected biodiversity information together with a logic and application layers allowing users to not only query the data but also discover additional facts of relevance implied by the data. We now proceed to specify the technologies that are used to implement the system.

\section{Choice of database paradigm for OpenBiodiv}

Taking into account the deliberations of the previous section we specified OpenBiodiv as a knowledge-based system with a focus on structuring and interlinking biodiversity data. Two possible technologies fit this requirement: semantic graph databases (RDF triple stores) such as GraphDB (\cite{ontotext_graphdb_2018}) and labeled property graphs such as Neo4J (\cite{neo4j_developers_neo4j_2012}).  Semantic graph databases offer a very simple data model: every fact stored in a such a database is composed as a triple of \emph{subject}, \emph{predicate}, and \emph{object}.  Subjects of triples are always resource identifiers, whereas objects can be other resource identfiers or literal values (e.g. strings, numbers, etc.).  Links between resources or between resources and literals are given by the predicates (also specified as identifiers). These links are sometimes referred to as \emph{properties}. Thus, one can visualize a graph formed whose verticies are the resource identifiers or literals and whose edges are the predicates.

Semantic graph databases have the unique feature that the logic layer is also expressed as triples stored in the database. This logic layer, known as \emph{ontology}, is not only responsible for drawing conclusions from the data (inference), but also specifies the semantics of how knowledge should be expressed.

Labeled property graphs, on the other hand, offer a more expressive data model by allowing the edges of the knowledge graph to have properties as well. For example, in a labeled property graph whose vertices are two cities A and B, connected by a property-predicate ``has road'', it is possible to additionally attach the value ``500 km'' to the property indicating the length of the road connecting the cities is 500 km. However, the semantics...

We settled on the first model---RDF triple store---due to the wide availability of high-quality ontologies and RDF data models in our domain (\cite{baskauf_darwin-sw:_2016,peroni_semantic_2014}) and the popularity of the Semantic Web (\cite{berners-lee_semantic_2001}) in the community. However, we believe that labeled property graphs are a more expressive model and are perfectly suited for relationships between taxonomic concepts (discussed in Chapter~\ref{chapter-ontology}). Also, non-RDF semantic databases such as \mbox{WikiData} are gaining in popularity. Therefore, we believe that the applicability of RDF triple stores for OpenBiodiv should constantly be reevaluated.

\section{Choice of Information Sources}

According to \cite{soraya_sierra_coordination_2014}, biodiversity and biodiversity-related data have two different ``life-cycles.'' In the past, after an observation of a living organism had been made, it was recorded on paper and then the observation record was published in paper-based form. In order for biodiversity data to be available to the modern scientist, efforts are made nowadays to digitize those paper-based publications by Plazi and the Biodiversity Heritage Library (\cite{agosti_why_2007,miller_taxonomic_2012}). For this purpose, several dedicated XML schemas have been developed (see \cite{penev_xml_2011} for a review), of which TaxPub and TaxonX  seem to be the most widely used (\cite{catapano_taxpub:_2010,penev_implementation_2012}). The digitization of those publications contains several steps. After scanning and optical character recognition (OCR), text mining is combined with searching for particular kinds of data. This procedure leaves a trace in the form of marked-up (tagged) elements that can then be extracted and made available for future use and reuse (\cite{miller_integrating_2015}).

At the time of writing, biodiversity data and publications are mostly ``born digital'' as semantically Enhanced Publications (EP's, \cite{claerbout_electronic_1992,godtsenhoven_van_emerging_2009,shotton_semantic_2009}). According to the first source, ``an EP is a publication that is enhanced with research data, extra materials, post publication data and database records. It has an object-based structure with explicit links between the objects. An object can be (part of) an article, a data set, an image, a movie, a comment, a module or a link to information in a database.''

Thus, the act of publishing in a digital, enhanced format, differs from the ground up from a paper-based publication. The main difference is that a digitally-published document can be structured in such a format as to be suitable both for machine processing and to the human eye. In the sphere of biodiversity science, Pensoft journals such as ZooKeys, PhytoKeys, and the Biodiversity Data Journal (BDJ) already function by providing EP's (\cite{penev_semantic_2010}).

Given the fact that Pensoft Publishers' and Plazi's publications cover a large part of taxonomic literature both in volume and also in temporal span, and the fact that the publications of those two publishers are available as semantic EP's, we've chosen Pensoft's journals and Plazi's treatments as our main sources of information.

Furthermore, we incorporate the taxonomic backbone of GBIF \cite{gbif_secretariat_gbif_2017} as a source for data integration. This is further discussed in Chapter~\ref{chapter-lod}.

\section{Choice of Programming Environment}

In recent years the R programming language has been used widely in the field of data science (\cite{r_core_team_r:_2016}). R has a rich library of software packages including such for processing XML (\cite{wickham_xml2:_2018}), for accessing rest API's (\cite{wickham_httr:_2017}) and focuses on Open Science (\todo{rOpenSci citation}). Furthermore, R is widely adopted in the biodiversity informatics community and the candidate is proficient in it. For this reason, we have chosen the R software environment as the main programming environment.

\section{Consideration of Open Science}

We are of the opinion that the OpenBiodiv needs to be addressed from the point of view of open science. According to \cite{kraker_case_2011} and to "Was ist Open Science?"\footnote{\href{http://openscienceasap.org/open-science/>}{<http://openscienceasap.org/open-science/>}}, the six principles of open science are open methodology, open source, open data, open access, open peer review, and open educational resources. It is our belief that the aim of open science is to ensure access to the whole research product: data, discoveries, hypotheses, and so on. This opening-up will ensure that the scientific product is reproducible and verifiable by other scientists (\cite{mietchen_transformative_2014}). There is a very high interest in development of processes and instruments enabling reproducibility and verifiability, as can be evidenced for example by a special issue in Nature dedicated to reproducible research (\cite{noauthor_challenges_2010}). Therefore, the source code, data, and publications of OpenBiodiv will be published openly.

\section{Definition of OpenBiodiv as a Research Problem}

Following the objectives outlined in the previous chapter and the clarification of the definitions from this chapter, the research problem of OpenBiodiv can postulated as designing an open RDF semantic graph database, incorporating information stored in Pensoft, Plazi, and GBIF, and allowing the user of the system to ask complicated queries. 

As a blueprint for the type queries in the domain of biodiversity science that should be answerable with the help of the system, we have looked at the list compiled in \cite{pro-ibiosphere_competency_2013}. Examples include: "Is X a valid taxonomic name?" ``What are related names to a given name?'' ``Which authors have published about a given taxon?''

In the next chapter we discuss the design and overall architecture of OpenBiodiv and in the subsequent chapters we discuss the individual components.
