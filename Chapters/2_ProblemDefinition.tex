\chapter{Problem Definition}
\label{chapter-problem-defintion}

As stated in Chapter~\ref{chapter-introduction}, the goal of the Ph.D. effort is "to create an open knowledge-based system of biodiversity information extracted from scholarly literature." As biodiversity data is quite heterogeneous and comes from many sources (see the domain conceptualization in Chapter~\ref{chapter-ontology}), the scope of the OpenBiodiv system developed as part of this Ph.D. effort is limited to providing a basis for the more general Open Biodiversity Knowledge Management System (OBKMS) by creating the models and infrastructure needed for processing scholarly publications of biological systematics and taxonomy. 

As per the "Principles of OBKMS" from \cite{pro-ibiosphere_open_2014}, the larger OBKMS ought to meet criteria such as providing "a consistent biodiversity information space," "new formats to support novel and diverse uses," "linkages with other resources," "accreditation for researcher's work," and others. Considerations for OBKMS were published in in the pro-iBiosphere final report (\cite{soraya_sierra_coordination_2014}). The language of the report is high-level and does not provide a formal definition for the system but rather a set of recommendations on the features and implementation of the system.

In 2016, based on the outcomes of pro-iBiosphere and on the main results in the area (see Chapter~\ref{chapter-introduction}), we published the Ph.D. plan for this thesis as a design specification for the system (\cite{senderov_open_2016}). However, in the course of developing the system, its design was changed iteratively through a feedback loop from collaborators from the BIG4 project consortium\footnote{The BIG4 project, \href{http://big4-project.eu}{<http://big4-project.eu>}.}, of which the author is a member of, and various international collaborators. We view this positively and in the spirit of both open Science (a discussion of open science follows later in the chapter) and the Manifesto for Agile Software Development\footnote{Manifesto for Agile Software Development \href{http://agilemanifesto.org/},{<http://agilemanifesto.org/>}.}.

Therefore, this chapter should serve as an updated version the Ph. D. project plan published in 2016 (\cite{senderov_open_2016}) and as a birds-eye of the OpenBiodiv system; subsequent chapters contain discussions of the particular components of the system.

\section{What is a knowledge-based system?}

It is useful to form one's concept of knowledge-based systems (KB\footnote{We use the moniker KB to denote ``knowledge base'' or the lengthly ``knowledge-based system'' when we want to emphasize aspects of the knowledge base that have nothing to do with underlying facts store.}) both by looking at some explicit definitions and by looking at several examples of knowledge bases in practice. The term was already widely discussed by the 1980's and early nineties (\cite{brodie_kbms_1989}) and was understood to mean the utilization of ideas from both database management systems (DBMS) and artificial intelligence (AI) to create a type of computer system called knowledge base management system (KBMS). The characteristics of such a system are that it contains ``prestored rules and facts from which useful inferences and conclusions may be drawn by an inference engine'' (\cite{harris_knowledge_1993}). We should note that the phrase ``prestored rules'' comes from the time of first-generation AI systems, which were rule based. Recently, there has been progress in incorporating statistical techniques into databases (\cite{mansinghka_bayesdb:_2015}); however, in this project we are working with the classical definition.

Another relatively more recent development in knowledge-based systems has been the application of Linked Data principles (\cite{heath_linked_2011}). In fact, most existing KB's emphasize the community aspects of KB's and making data more interconnect and reusable. Examples include Freebase (\cite{bollacker_freebase:_2008}), which recently migrated to WikiData (\cite{vrandecic_wikidata:_2014, pellissier_tanon_freebase_2016}), and DBPedia (\cite{hutchison_dbpedia:_2007}) to name a few. We will discuss the Linked Data principles and their application to OpenBiodiv in Chapter~\ref{chapter-lod}. Together with the push to interlink data, there has been critique of the idea of bundling logic in the database layer as such bundling leads to increased complexity (\cite{barrasa_rdf_2017}). This leads to an interesting conundrum in the choice of a database technology discussed later in this chapter.

\section{Choice of a Database Paradigm}

Taking into account the deliberations of the previous section we specified OpenBiodiv as a light-weight system with a focus on structuring and interlinking biodiversity data rather than complicated inference. Two possible technologies fit this requirement: semantic graph databases (RDF triple stores) such as GraphDB (\cite{ontotext_graphdb_2018}) and labeled property graphs such as Neo4J (\cite{neo4j_developers_neo4j_2012}). We settled on the first model---RDF triple store---due to the wide availability of high-quality ontologies and RDF data models in our domain (\cite{baskauf_darwin-sw:_2016,peroni_semantic_2014}) and the popularity of the Semantic Web (\cite{berners-lee_semantic_2001}) in the community. However, we believe that labeled property graphs are a more expressive model and are perfectly suited for relationships between taxonomic concepts (discussed in Chapter~\ref{chapter-ontology}). Also, non-RDF semantic databases such as \mbox{WikiData} are gaining in popularity. Therefore, we believe that the applicability of RDF triple stores for OpenBiodiv should constantly be reevaluated.

\section{Choice of Information Sources}

According to \cite{soraya_sierra_coordination_2014}, biodiversity and biodiversity-related data have two different ``life-cycles.'' In the past, after an observation of a living organism had been made, it was recorded on paper and then the observation record was published in paper-based form. In order for biodiversity data to be available to the modern scientist, efforts are made nowadays to digitize those paper-based publications by Plazi and the Biodiversity Heritage Library (\cite{agosti_why_2007,miller_taxonomic_2012}). For this purpose, several dedicated XML schemas have been developed (see \cite{penev_xml_2011} for a review), of which TaxPub and TaxonX  seem to be the most widely used (\cite{catapano_taxpub:_2010,penev_implementation_2012}). The digitization of those publications contains several steps. After scanning and optical character recognition (OCR), text mining is combined with searching for particular kinds of data. This procedure leaves a trace in the form of marked-up (tagged) elements that can then be extracted and made available for future use and reuse (\cite{miller_integrating_2015}).

At the time of writing, biodiversity data and publications are mostly ``born digital'' as semantically Enhanced Publications (EP's, \cite{claerbout_electronic_1992,godtsenhoven_van_emerging_2009,shotton_semantic_2009}). According to the first source, ``an EP is a publication that is enhanced with research data, extra materials, post publication data and database records. It has an object-based structure with explicit links between the objects. An object can be (part of) an article, a data set, an image, a movie, a comment, a module or a link to information in a database.''

Thus, the act of publishing in a digital, enhanced format, differs from the ground up from a paper-based publication. The main difference is that a digitally-published document can be structured in such a format as to be suitable both for machine processing and to the human eye. In the sphere of biodiversity science, Pensoft journals such as ZooKeys, PhytoKeys, and the Biodiversity Data Journal (BDJ) already function by providing EP's (\cite{penev_semantic_2010}).

Given the fact that Pensoft Publishers' and Plazi's publications cover a large part of taxonomic literature both in volume and also in temporal span, and the fact that the publications of those two publishers are available as semantic EP's, we've chosen Pensoft's journals and Plazi's treatments as our main sources of information.

Furthermore, we incorporate the taxonomic backbone of GBIF \cite{gbif_secretariat_gbif_2017} as a source for data integration. This is further discussed in Chapter~\ref{chapter-lod}.

\section{Choice of Programming Environment}

In recent years the R programming language has been used widely in the field of data science (\cite{r_core_team_r:_2016}). R has a rich library of software packages including such for processing XML (\cite{wickham_xml2:_2018}), for accessing rest API's (\cite{wickham_httr:_2017}) and focuses on Open Science (\todo{rOpenSci citation}). Furthermore, R is widely adopted in the biodiversity informatics community and the candidate is proficient in it. For this reason, we have chosen the R software environment as the main programming environment.

\section{Consideration of Open Science}

We are of the opinion that the OpenBiodiv needs to be addressed from the point of view of open science. According to \cite{kraker_case_2011} and to "Was ist Open Science?"\footnote{\href{http://openscienceasap.org/open-science/>}{<http://openscienceasap.org/open-science/>}}, the six principles of open science are open methodology, open source, open data, open access, open peer review, and open educational resources. It is our belief that the aim of open science is to ensure access to the whole research product: data, discoveries, hypotheses, and so on. This opening-up will ensure that the scientific product is reproducible and verifiable by other scientists (\cite{mietchen_transformative_2014}). There is a very high interest in development of processes and instruments enabling reproducibility and verifiability, as can be evidenced for example by a special issue in Nature dedicated to reproducible research (\cite{noauthor_challenges_2010}). Therefore, the source code, data, and publications of OpenBiodiv will be published openly.

\section{Definition of OpenBiodiv as a Research Problem}

Following the objectives outlined in the previous chapter and the clarification of the definitions from this chapter, the research problem of OpenBiodiv can postulated as designing an open RDF semantic graph database, incorporating information stored in Pensoft, Plazi, and GBIF, and allowing the user of the system to ask complicated queries. 

As a blueprint for the type queries in the domain of biodiversity science that should be answerable with the help of the system, we have looked at the list compiled in \cite{pro-ibiosphere_competency_2013}. Examples include: "Is X a valid taxonomic name?" ``What are related names to a given name?'' ``Which authors have published about a given taxon?''

In the next chapter we discuss the design and overall architecture of OpenBiodiv and in the subsequent chapters we discuss the individual components.